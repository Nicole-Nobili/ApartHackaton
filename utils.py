import goodfire
from typing import List
import re
from prompts import JUDGE_SYSTEM_PROMPT
from goodfire import FeatureGroup
import os


class Retriever:
    def __init__(self, client: goodfire.Client, variant: str):
        self.client = client
        self.variant = goodfire.Variant(variant)

    def retrieve_features(self, target_behavior: str):
        """Retrieve features relevant to a given prompt.

        Args:
            target_behavior (str): The desired behavior to evaluate against
        Returns:
            List[features]: List of feature values retrieved from the search
        """
        pirate_features, relevance = self.client.features.search(
            target_behavior,
            model=self.variant,
            top_k=5
        )
        return pirate_features

class Scorer:
    def __init__(self, client: goodfire.Client, variant: str):
        self.client = client
        self.variant = goodfire.Variant(variant)

    def parseStrToList(self, score_gen: str):
        numbers = re.findall(r'-?\d*\.?\d+', score_gen)
        weights = [float(x) for x in numbers]  # Convert strings to floats
        return weights

    
    def score_features(self, target_behavior: str, critique: str, features, prev_scores):
        """Score a list of features to get their weights.

        Args:
            target_behavior (str): The desired behavior to evaluate against
            critique (str): A free-text critique evaluating how well the steered 
                output matches the target behavior given the input
            features (List[float]): List of feature values to score

        Returns:
            List[float]: Ordered list of feature weights between -1 and 1
        """
        
        if (critique is None or len(critique) == 0):
            critique = f"""Your previous output scores are {prev_scores}, and the feedback is {critique}, adjust the score to improve critique."""
        else:
            critique = ""
        self.SYS_PROMPT = f"""
Instruction for the assistant:
You are tasked with {target_behavior}
{critique}
Your role is to output a List of scores between -1 and 1 when given a python List of features. Two lists should have the same length.
Example:
    User: FeatureGroup([0: "The user is directing the model to modify or transform content", [1: "The model is being instructed how to answer",].\
    System Response: [0.3, -0.7]
"""
        score_gen = ""
        for token in client.chat.completions.create(
            [
                {"role": "system", "content": self.SYS_PROMPT},
                {"role": "user", "content": f"{str(features)}"}
            ],
            model=self.variant,
            stream=True,
            max_completion_tokens=50,
        ):
            score_gen += token.choices[0].delta.content
        print(f"{score_gen=}")
        weights = self.parseStrToList(score_gen)
        while len(weights) != len(features):
            score_gen = ""
            for token in client.chat.completions.create(
                [
                    {"role": "system", "content": self.SYS_PROMPT},
                    {"role": "user", "content": f"You didn't give me score or \
                        the score does not have right length, please give me a list of scores based on \{str(features)}"}
                ],
                model=self.variant,
                stream=True,
                max_completion_tokens=50,
            ):
                score_gen += token.choices[0].delta.content
            print(f"{score_gen=}")
            weights = self.parseStrToList(score_gen)

        return weights

class Judge:
    def __init__(self, client: goodfire.Client, variant: str):  
        self.client = client
        self.variant = goodfire.Variant(variant)
        self.SYS_PROMPT = JUDGE_SYSTEM_PROMPT
    
    def judge_output(self, target_behavior: str, steered_model_output: str, steered_model_input: str):
        """Judge a steered model output against a target behavior.

        Args:
            target_behavior (str): The desired behavior to evaluate against
            steered_model_output (str): The output generated by the steered model
            steered_model_input (str): The input provided to the steered model
                (Can be a list of various inputs and outputs)

        Returns:
            str: A free-text critique evaluating how well the steered output
                matches the target behavior given the input
        """
        #maybe look at textgrad loss?
        completion = ""
        for token in self.client.chat.completions.create(
            [
                {"role": "system", "content": self.SYS_PROMPT},
                {"role": "user", "content": f"""Input prompt:\n{steered_model_input}\n\nResponse:\n{steered_model_output}\n\nTarget Behavior:{target_behavior}\n\n"""}
            ],
            model=self.variant,
            stream=True,
            max_completion_tokens=200,
        ):
            completion += token.choices[0].delta.content
        return completion

class SteeredModel:
    def __init__(self, client: goodfire.Client, variant: str):
        self.client = client
        self.variant = goodfire.Variant(variant)
        
    def set_features(self, features, scores: List[float]):
        """Set the features and scores for the steered model.

        Args:
            features (FeatureGroup): List of feature values
            scores (List[float]): List of scores between -1 and 1 for the corresponding features
        """
        print(f"{features=}, {scores=}")

        assert len(features) == len(scores)
        self.variant.reset()
        for feature, score in zip(features, scores):
            self.variant.set(feature, score)
    
    def generate(self, prompt: str):
        """Generate a response from the steered model.

        Args:
            prompt (str): The input prompt to the model
        """
        completion = ""
        for token in self.client.chat.completions.create(
            [
                {"role": "user", "content": prompt}
            ],
            model=self.variant,
            stream=True,
            max_completion_tokens=200,
        ):
            completion += token.choices[0].delta.content
        return completion

if __name__ == "__main__":
    GOODFIRE_API_KEY = ''# os.environ.get('GOODFIRE_API_KEY')
    client = goodfire.Client(GOODFIRE_API_KEY)
    TARGET_BEHAVIOR = "Be good at math."
    EPOCHS = 10
    PROMPT = "Which one is bigger, 9.9 or 9.11?" #for now fixed
    
    retriever = Retriever(client, "meta-llama/Meta-Llama-3-8B-Instruct")
    scorer = Scorer(client, "meta-llama/Meta-Llama-3-8B-Instruct")
    judge = Judge(client, "meta-llama/Meta-Llama-3-8B-Instruct")
    steered_model = SteeredModel(client, "meta-llama/Meta-Llama-3-8B-Instruct")
    model_output = steered_model.generate(PROMPT)
    print(f"{model_output=}")
   
    features = retriever.retrieve_features(TARGET_BEHAVIOR)
    scores = scorer.score_features(TARGET_BEHAVIOR, "", features, [])
    steered_model.set_features(features, scores)
    model_output = steered_model.generate(PROMPT)
    for i in range(EPOCHS):
        print(f"Epoch {i}")
        critique = judge.judge_output(TARGET_BEHAVIOR, model_output, PROMPT)
        scores = scorer.score_features(TARGET_BEHAVIOR, critique, features, scores)
        steered_model.set_features(features, scores)
        model_output = steered_model.generate(PROMPT)
    
        print(f"{PROMPT=}")
        print(f"{model_output=}")
        print(f"{critique=}")
